#!/usr/bin/python3
# -*-coding:utf-8 -*


import logging
import argparse

import os
import sys
import yaml
import itertools

lib_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '../lib'))
sys.path.append(lib_path)

from xi.ml.common import Timer
from xi.ml.error import ConfigError, CaughtException
from xi.ml.tools import utils, PathGenerator
from xi.ml.corpus import dictionary, pickler, LoadCorpora
from xi.ml.transform import TrainTransformer, LoadTransformer, Topics
from xi.ml.classify import TrainClassifier, LoadClassifier, \
    PredictionStatistics, EvalMetrics

#=============================================
# Parse the command line arguments
#=============================================

options = {}

parser = argparse.ArgumentParser(
    description='Train and test models based on requests')
parser.add_argument('conf', help='input config file (yml)')
options = parser.parse_args()

# check config file existance
if not os.path.isfile(options.conf):
    raise ConfigError("Config file '{}' not found".format(options.conf))

#=============================================
# Logger setup
#=============================================

logger = logging.getLogger('xi.ml')

# logger setup for gensim
logging.basicConfig(
    format='[%(name)s] [%(asctime)s] %(levelname)s : %(message)s',
    level=logging.INFO)

#=============================================
# Load configuration
#=============================================

conf = {}

with open(options.conf, 'r') as stream:
    try:
        conf = yaml.load(stream)
    except yaml.YAMLError as exc:
        raise CaughtException(
            "Exception encountered during YAML load: {}".format(exc))

if not isinstance(conf, dict):
    raise ConfigError("Not a dict object stored in '{}'".format(options.conf))

if not conf:
    raise ConfigError("Empty configuration in '{}'".format(options.conf))

#=============================================
# Configuration checkups
#=============================================

for key in ['classes', 'res', 'execution']:
    if key not in conf:
        raise ConfigError("Missing mandatory config option '{}'".format(key))

if not os.path.isdir(conf['res']):
    raise ConfigError("Data folder '{}' not found".format(conf['res']))

logger.info("Current configuration: {}".format(conf))

# set default filter for the word dictionary
filter_dict = { 'no_below': 5, 'no_above': 0.75, 'keep_n': 500000 }
if 'dictionary' in conf and isinstance(conf['dictionary'], dict):
    filter_dict = dict(conf['dictionary'])

#=============================================
# Set local ressources configuration
# - where to save the data
# - where to save the models
# - ...
#=============================================

subsets = conf.get('subsets', ['train', 'dev', 'test'])
preprocessings = conf.get('preprocessings', [])
transformations = conf.get('transformations', {})
classifiers = conf.get('classifiers', {})


local = PathGenerator(
    conf['res'], conf['classes'], subsets,
    preprocessings, transformations.keys(), classifiers.keys())

#=============================================
# Train transformation models
#=============================================

timer = Timer()

if 'train_trans' in conf['execution']:
    params = {
        'transformation': transformations.keys(),
        'preprocessing': preprocessings
    }

    for combination in itertools.product(*params.values()):
        dict_comb = dict(zip(params.keys(), combination))
        trans = dict_comb['transformation']
        preproc = dict_comb['preprocessing']

        logger.info("Training '{}-{}' model".format(trans, preproc))

        # training options for current transformation
        trans_opts = transformations[trans]

        # list of training files
        train_files = local.preprocessed_files(preproc, 'train')

        # files for dictionary, bow corpus, tf-idf corpus, transformation model
        dict_file = local.dictionary(preproc)
        dict_file_text = utils.change_extension(dict_file, 'txt')
        dict_file_json = utils.change_extension(dict_file, 'json')
        bow_file = local.transformation_model('bow', trans, preproc)

        tfidf_file = local.transformation_model('TFIDF', trans, preproc)
        tfidf_shape = utils.change_extension(tfidf_file, 'json')

        trans_file = local.transformation_model(trans, trans, preproc)
        trans_shape = utils.change_extension(trans_file, 'json')

        if not os.path.exists(dict_file) or not os.path.exists(bow_file):
            #-------------------------------------------
            # set up, filter and save the dictionary
            #-------------------------------------------

            logger.info("Create a new corpus on '{}' files".format(train_files))
            timer.start_timer()

            corpus = LoadCorpora(train_files)

            logger.info("Filter the dictionary using the {} rules".format(
                filter_dict))

            corpus.dictionary.filter_extremes(**filter_dict)

            utils.create_path(dict_file)
            corpus.dictionary.save(dict_file)
            corpus.dictionary.save_as_text(dict_file_text)
            dictionary.load_and_save_as_json(dict_file, dict_file_json)

            timer.stop_timer('Dictionary created and filtered')

            #-------------------------------------------
            # set the bow representation
            # (executed when iterating over the corpus)
            #-------------------------------------------

            logger.info('Set up and save the bow representation')
            timer.start_timer()

            pickler.save(bow_file, corpus, 10000)
            timer.stop_timer('BOW transformation executed')

        #-------------------------------------------
        # train the transformation model
        #-------------------------------------------

        logger.info('Reload dictionary and bow corpus')
        bow_corpus = pickler.load(bow_file)
        dictionary = dictionary.load(dict_file)

        if trans == "LDA":
            logger.info('Train the LDA transformation model')

            timer.start_timer()
            model = TrainTransformer(trans, trans_opts)
            model.train(bow_corpus, dictionary)
            model.save(trans_file)
            timer.stop_timer('LDA transformation trained')
        else:
            if not os.path.exists(tfidf_file):
                logger.info('Train the tf-idf transformation model')

                timer.start_timer()
                tfidf_model = TrainTransformer('TFIDF')
                tfidf_model.train(bow_corpus)
                tfidf_model.save(tfidf_file)
                tfidf_model.save_shape(tfidf_shape)
                timer.stop_timer('TFIDF transformation executed')
            else:
                tfidf_model = LoadTransformer('TFIDF', tfidf_file)

            # transform bow_corpus into tfidf_corpus
            tfidf_corpus = tfidf_model.model[bow_corpus]

            # train the model
            logger.info("Train the {} transformation model".format(trans))

            timer.start_timer()
            model = TrainTransformer(trans, **trans_opts)
            model.train(tfidf_corpus, dictionary)
            model.save(trans_file)
            model.save_shape(trans_shape)
            timer.stop_timer("{} transformation trained".format(trans))

#=============================================
# Get topics
#=============================================

if 'get_topics' in conf['execution']:

    params = {
        'transformation': set(['LSI', 'LDA']).intersection(
            transformations.keys()),
        'preprocessing': preprocessings
    }

    for combination in itertools.product(*params.values()):
        dict_comb = dict(zip(params.keys(), combination))
        trans = dict_comb['transformation']
        preproc = dict_comb['preprocessing']

        mfile = local.transformation_model(trans, trans, preproc)
        tfile = local.transformation_topics(trans, preproc)
        tposfile = utils.change_extension(tfile, 'pos.json')

        if os.path.exists(mfile):
            logger.info("Get topics from '{}-{}' model".format(trans, preproc))

            topics = Topics(trans, mfile)
            topics.save(tfile, 50)
            topics.save_positives(tposfile, 50)

#=============================================
# Transform data
#=============================================

if 'transform_data' in conf['execution']:

    params = {
        'transformation': transformations.keys(),
        'preprocessing': preprocessings
    }

    for combination in itertools.product(*params.values()):
        dict_comb = dict(zip(params.keys(), combination))
        trans = dict_comb['transformation']
        preproc = dict_comb['preprocessing']

        # dictionary's file, tf-idf file, current transformation model's file
        dict_file = local.dictionary(preproc)
        tfidf_file = local.transformation_model('TFIDF', trans, preproc)
        trans_file = local.transformation_model(trans, trans, preproc)

        if os.path.exists(dict_file) and os.path.exists(trans_file):

            # load current transformation model
            model = LoadTransformer(trans, trans_file)

            sparams = {
                'category': conf['classes'],
                'subset': subsets,
            }

            for scombination in itertools.product(*sparams.values()):
                sub_dict_comb = dict(zip(sparams.keys(), scombination))
                category = sub_dict_comb['category']
                subset = sub_dict_comb['subset']

                logger.info(
                    "Data transformation: '{}-{}' model on '{}-{}' corpus"
                    .format(trans, preproc, category, subset))

                # input: preprocessed data file; output: transformed data file
                ifn = local.preprocessed_file(category, subset, preproc)
                ofn = local.transformed_file(category, subset, trans, preproc)

                timer.start_timer()
                if trans == 'lda':
                    model.store_transformation(ifn, ofn, dict_file, None)
                else:
                    model.store_transformation(ifn, ofn, dict_file, tfidf_file)
                timer.stop_timer("{} transformed execution".format(trans))

#=============================================
# Train document classifiers
#=============================================

if 'train_classifiers' in conf['execution']:

    for classif_name, options in classifiers.items():

        params = {
            'transformation':  transformations.keys(),
            'preprocessing': preprocessings,
            'classif_type': options['classif_types'],
            'train_type': options['train_types'],
            'chunk_size': options['chunk_sizes']
        }

        # optional initialization arguments for a given classifier
        kwargs = options.get('kwargs', {})

        for combination in itertools.product(*params.values()):
            dict_comb = dict(zip(params.keys(), combination))
            trans = dict_comb['transformation']
            preproc = dict_comb['preprocessing']
            classif_type = dict_comb['classif_type']
            train_type = dict_comb['train_type']
            chunk_size = dict_comb['chunk_size']

            logger.info(
                "Train classification model: {}-{}-{}-{}"
                .format(classif_name, classif_type, train_type, chunk_size))

            # set file names
            train_files = local.transformed_files(trans, preproc, 'train')

            model_file = local.classification_model(
                trans, preproc,
                classif_name, classif_type, train_type, chunk_size)

            shape_file = utils.change_extension(model_file, 'json')

            # initialize classifier
            classifier = TrainClassifier(
                classif_name, classif_type, conf['classes'], **kwargs)

            # train classifier
            classifier.train(train_type, train_files, chunk_size)

            # save classifier
            classifier.save(model_file)
            classifier.save_shape(shape_file)

#=============================================
# Test document classifiers
#=============================================

if 'test_classifiers' in conf['execution']:

    for classif_name, options in classifiers.items():

        params = {
            'transformation':  transformations.keys(),
            'preprocessing': preprocessings,
            'classif_type': options['classif_types'],
            'train_type': options['train_types'],
            'chunk_size': options['chunk_sizes']
        }

        for combination in itertools.product(*params.values()):
            dict_comb = dict(zip(params.keys(), combination))
            trans = dict_comb['transformation']
            preproc = dict_comb['preprocessing']
            classif_type = dict_comb['classif_type']
            train_type = dict_comb['train_type']
            chunk_size = dict_comb['chunk_size']

            #-------------------------------------------
            # set file names
            #-------------------------------------------

            model_file = local.classification_model(
                trans, preproc,
                classif_name, classif_type, train_type, chunk_size)

            # own stats file (.json)
            stats_file = local.stats_file(
                classif_name, classif_type, train_type, chunk_size,
                trans, preproc)

            # roc & plot files
            roc_file = utils.change_extension(stats_file, 'roc')
            plot_file = utils.change_extension(stats_file, 'png')

            #-------------------------------------------
            # use and evaluate classifier
            #-------------------------------------------

            if os.path.exists(model_file):

                # load classifier
                classifier = LoadClassifier(model_file)

                # classify documents and store predictions
                for category in conf['classes']:

                    ifn = local.transformed_file(
                        category, 'test', trans, preproc)

                    ofn = local.classified_file(
                        category, 'test',
                        classif_name, classif_type, train_type, chunk_size,
                        trans, preproc)

                    classifier.store_prediction(ifn, ofn)

                # filenames of classified documents
                test_files = local.classified_files(
                    classif_name, classif_type, train_type, chunk_size,
                    trans, preproc, 'test')

                #-------------------------------------------
                # compute own global statistics
                #-------------------------------------------

                sprediction = PredictionStatistics(test_files, conf['classes'])
                sprediction.compute_stats()
                sprediction.store_stats(stats_file)

                #-------------------------------------------
                # compute sklearn statistics
                #-------------------------------------------

                evaluation = EvalMetrics(test_files, conf['classes'])

                # compute the global confusion matrix
                evaluation.confusion_matrix()

                # compute the acc, auc, roc curve for each category
                evaluation.store_stats(roc_file, plot_file)
